# app/news_professor.py
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Iterable, List, Optional, Tuple

from .config import settings
from .db import (
    get_news_by_urls,
    init_db,
    link_exists,
    save_news,
)
from .filters import filter_link_by_substring
from .link_extractor import extract_links_from_url
from .logging_utils import log_error, log_info, log_warning
from .scoring import compute_tfidf_scores
from .telegram_bot import format_news_message, send_message
from .text_parser import fetch_text_content

# ---------- –ù–∞–±–æ—Ä—ã —Å–∞–π—Ç–æ–≤ –ø–æ–¥ —Ç–µ–º–∞—Ç–∏–∫—É ----------

# –ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ ‚Äî AI / –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
SITES_AI: List[str] = [
    "https://openai.com",
    "https://blog.google/technology/ai/",
    "https://www.anthropic.com/news",
    "https://huggingface.co/blog",
    "https://stability.ai/news",
]

# –í—Ç–æ—Ä–Ω–∏–∫ ‚Äî Python
SITES_PYTHON: List[str] = [
    "https://www.python.org/blogs/",
    "https://realpython.com/tutorials/news/",
    "https://blog.jetbrains.com/pycharm/",
    "https://www.pythonweekly.com/",
]

# –°—Ä–µ–¥–∞ ‚Äî Data Engineering / Big Data
SITES_DATA_ENG: List[str] = [
    "https://www.databricks.com/blog",
    "https://www.confluent.io/blog/",
    "https://aws.amazon.com/blogs/big-data/",
]

# –ß–µ—Ç–≤–µ—Ä–≥ ‚Äî Security
SITES_SECURITY: List[str] = [
    "https://thehackernews.com/",
    "https://gbhackers.com/",
    "https://cybersecuritynews.com/",
]

# –ü—è—Ç–Ω–∏—Ü–∞ ‚Äî DevTools / —Å–µ—Ä–≤–∏—Å—ã
SITES_DEVTOOLS: List[str] = [
    "https://github.blog/news-insights/",
    "https://code.visualstudio.com/updates",
    "https://www.docker.com/blog/",
]

# –°—É–±–±–æ—Ç–∞ ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞ —Ç—É–ª–∑–æ–≤ (DevTools + Python)
SITES_TOOLS_DAY: List[str] = SITES_DEVTOOLS + SITES_PYTHON

# –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç (–≤—Å—ë –ø–æ–¥—Ä—è–¥)
ALL_SITES: List[str] = SITES_AI + SITES_PYTHON + SITES_DATA_ENG + SITES_SECURITY + SITES_DEVTOOLS


@dataclass
class ContentPlanConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥ –Ω–∞ –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏:
    - sites: –æ—Ç–∫—É–¥–∞ —Ç—è–Ω–µ–º –Ω–æ–≤–æ—Å—Ç–∏
    - substring: —Ñ–∏–ª—å—Ç—Ä –ø–æ URL (–æ–±—ã—á–Ω–æ /2025/)
    - max_fetch: —Å–∫–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º—É–º –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ —Ä–∞–∑ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    """

    sites: List[str]
    substring: str
    max_fetch: int


# 0 = –ü–Ω, 6 = –í—Å
CONTENT_PLAN: Dict[int, ContentPlanConfig] = {
    0: ContentPlanConfig(  # –ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ ‚Äî AI/–Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        sites=SITES_AI,
        substring="/2025/",
        max_fetch=40,
    ),
    1: ContentPlanConfig(  # –í—Ç–æ—Ä–Ω–∏–∫ ‚Äî Python
        sites=SITES_PYTHON,
        substring="/2025/",
        max_fetch=40,
    ),
    2: ContentPlanConfig(  # –°—Ä–µ–¥–∞ ‚Äî Data Engineering
        sites=SITES_DATA_ENG,
        substring="/2025/",
        max_fetch=40,
    ),
    3: ContentPlanConfig(  # –ß–µ—Ç–≤–µ—Ä–≥ ‚Äî Security
        sites=SITES_SECURITY,
        substring="/2025/",
        max_fetch=40,
    ),
    4: ContentPlanConfig(  # –ü—è—Ç–Ω–∏—Ü–∞ ‚Äî DevTools / —Å–µ—Ä–≤–∏—Å—ã
        sites=SITES_DEVTOOLS,
        substring="/2025/",
        max_fetch=40,
    ),
    5: ContentPlanConfig(  # –°—É–±–±–æ—Ç–∞ ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞ —Ç—É–ª–∑–æ–≤
        sites=SITES_TOOLS_DAY,
        substring="/2025/",
        max_fetch=60,
    ),
    6: ContentPlanConfig(  # –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ–¥–µ–ª–∏
        sites=ALL_SITES,
        substring="/2025/",
        max_fetch=80,
    ),
}


DAY_TOPIC_TAGS: Dict[int, Dict[str, str]] = {
    0: {"topic_tag": "#AI", "source_tag": "#–ù–µ–π—Ä–æ—Å–µ—Ç–∏"},  # –ü–Ω
    1: {"topic_tag": "#Python", "source_tag": "#–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞"},  # –í—Ç
    2: {"topic_tag": "#DataEngineering", "source_tag": "#BigData"},  # –°—Ä
    3: {"topic_tag": "#Security", "source_tag": "#DevSecOps"},  # –ß—Ç
    4: {"topic_tag": "#Tools", "source_tag": "#DevTools"},  # –ü—Ç
    5: {"topic_tag": "#Tools", "source_tag": "#–ü–æ–¥–±–æ—Ä–∫–∞"},  # –°–±
    6: {"topic_tag": "#Digest", "source_tag": "#–î–∞–π–¥–∂–µ—Å—Ç"},  # –í—Å
}


def get_today_tags() -> Dict[str, str]:
    weekday = datetime.now().weekday()
    return DAY_TOPIC_TAGS.get(weekday, {"topic_tag": "#–ù–æ–≤–æ—Å—Ç–∏IT", "source_tag": "#IT"})


def guess_source_from_url(url: str) -> str:
    url = url.lower()

    # AI
    if "openai.com" in url:
        return "openai"
    if "blog.google/technology/ai" in url:
        return "google_ai_blog"
    if "anthropic.com" in url:
        return "anthropic"
    if "huggingface.co" in url:
        return "huggingface"
    if "stability.ai" in url:
        return "stability_ai"

    # Python
    if "python.org" in url:
        return "python_org"
    if "realpython.com" in url:
        return "realpython"
    if "blog.jetbrains.com/pycharm" in url:
        return "pycharm_blog"
    if "pythonweekly.com" in url:
        return "python_weekly"

    # Data Engineering
    if "databricks.com" in url:
        return "databricks"
    if "confluent.io" in url:
        return "confluent"
    if "aws.amazon.com/blogs/big-data" in url:
        return "aws_bigdata"

    # Security
    if "thehackernews.com" in url:
        return "the_hacker_news"
    if "gbhackers.com" in url:
        return "gbhackers"
    if "cybersecuritynews.com" in url:
        return "cybersecuritynews"

    # DevTools
    if "github.blog" in url:
        return "github_blog"
    if "code.visualstudio.com/updates" in url:
        return "vscode_updates"
    if "docker.com/blog" in url:
        return "docker_blog"

    return "other"


def split_title_and_summary(content: str) -> Tuple[Optional[str], Optional[str]]:
    """
    –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ title, —Å–ª–µ–¥—É—é—â–∏–µ 2‚Äì3 —Å—Ç—Ä–æ–∫–∏ —Å–∫–ª–µ–∏–≤–∞–µ–º –≤ summary.
    """
    lines = [line.strip() for line in content.splitlines() if line.strip()]
    if not lines:
        return None, None

    title = lines[0][:200]
    body_lines = lines[1:4]  # 2‚Äì3 —Å—Ç—Ä–æ–∫–∏
    summary = " ".join(body_lines)[:600] if body_lines else None
    return title, summary


def build_tool_use_case(source: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–ª—è —é–∑–∫–µ–π—Å–∞.
    –ú–æ–∂–Ω–æ –ø–æ—Ç–æ–º —É—Å–ª–æ–∂–Ω–∏—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
    """
    src = (source or "other").lower()

    if src in {"github_blog"}:
        return (
            "–°–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ GitHub "
            "–∏ —É–ª—É—á—à–∞—Ç—å —Å–≤–æ–π workflow —Å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è–º–∏ –∏ CI/CD."
        )

    if src in {"vscode_updates"}:
        return "–ü–æ–ª—É—á–∞—Ç—å –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ –≤ VS Code " "–∏ –ø—Ä–æ–∫–∞—á–∏–≤–∞—Ç—å —É–¥–æ–±—Å—Ç–≤–æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –∫–æ–¥–∏–Ω–≥–∞."

    if src in {"docker_blog"}:
        return "–£–ø—Ä–æ—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π " "–∏ —Ä–∞–±–æ—Ç—É —Å –æ–∫—Ä—É–∂–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ Docker."

    if src in {"python_org", "realpython", "pycharm_blog", "python_weekly"}:
        return "–ü—Ä–æ–∫–∞—á–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –Ω–∞ Python " "–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã."

    if src in {"databricks", "confluent", "aws_bigdata"}:
        return "–£–ø—Ä–æ—Å—Ç–∏—Ç—å —Ä–∞–±–æ—Ç—É —Å data-–ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏, —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º " "–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."

    # üî• –í–ê–ñ–ù–û: default –≤–∞—Ä–∏–∞–Ω—Ç –í–°–ï–ì–î–ê –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–º return
    return "–ü–æ–º–æ–∂–µ—Ç —É–ø—Ä–æ—Å—Ç–∏—Ç—å –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é " "—Ä–∞–±–æ—Ç—É —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –∏ —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –≤—Ä–µ–º—è."


class NewsProfessor:
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä:
    - —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω—É
    - –ø–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç
    - —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ (TF-IDF + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
    - –ø–∏—à–µ—Ç –≤ –ë–î
    - –ø—É–±–ª–∏–∫—É–µ—Ç —Ç–æ–ø –≤ Telegram
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        init_db(self.db_path)

    def collect_links(self, sites: Iterable[str]) -> List[str]:
        all_links: List[str] = []
        for site in sites:
            try:
                log_info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Å—ã–ª–∫–∏ —Å {site}")
                links = extract_links_from_url(site)
                log_info(f"{site}: –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")
                all_links.extend(links)
            except Exception as e:
                log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {site}: {e}", alert=True)
        return all_links

    def fetch_and_store_new_articles_batch(
        self,
        links: Iterable[str],
        substring: str,
        max_to_fetch: int,
    ) -> List[str]:
        """
        - —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, /2025/)
        - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ, —á—Ç–æ —É–∂–µ –µ—Å—Ç—å –≤ –ë–î
        - –ø–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç, —Å—á–∏—Ç–∞–µ–º TF-IDF score, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL-–æ–≤ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π.
        """
        filtered_links = filter_link_by_substring(links, substring)
        log_info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ '{substring}' –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_links)} —Å—Å—ã–ª–æ–∫")

        new_articles: List[Tuple[str, str, Optional[str], str, str]] = []
        # (url, title, summary, content, source)

        for url in filtered_links:
            if len(new_articles) >= max_to_fetch:
                break

            if link_exists(self.db_path, url):
                continue

            try:
                content = fetch_text_content(url)
            except Exception as e:
                log_error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}", alert=False)
                continue

            if not content:
                continue

            title, summary = split_title_and_summary(content)
            source = guess_source_from_url(url)
            new_articles.append((url, title or "", summary or "", content, source))

        if not new_articles:
            log_info("–ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ—Ç.")
            return []

        # TF-IDF —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        texts_for_scoring = [
            f"{title}\n{summary}\n{content}" for _, title, summary, content, _ in new_articles
        ]
        scores = compute_tfidf_scores(texts_for_scoring)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        for (url, title, summary, content, source), score in zip(new_articles, scores):
            save_news(
                self.db_path,
                url=url,
                title=title,
                summary=summary,
                content=content,
                source=source,
                score=score,
            )
            log_info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {url} (score={score:.3f})")

        return [url for url, *_ in new_articles]

    def publish_top_news(self, new_urls: List[str], max_to_publish: int = 5) -> None:

        if not new_urls:
            log_info("–ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç, –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –Ω–µ—á–µ–≥–æ.")
            return

        rows = get_news_by_urls(self.db_path, new_urls)
        if not rows:
            log_warning("get_news_by_urls –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
            return

        # rows: (url, title, summary, content, source, score)
        scored = []
        for url, title, summary, content, source, score in rows:
            scored.append((score or 0.0, url, title, summary, content, source))

        scored.sort(reverse=True, key=lambda x: x[0])
        top = scored[:max_to_publish]

        tags = get_today_tags()
        topic_tag = tags["topic_tag"]
        default_source_tag = tags["source_tag"]

        for (
            score,
            url,
            title,
            summary,
            content,
            source,
        ) in top:
            if source in {"openai", "anthropic", "huggingface", "stability_ai", "google_ai_blog"}:
                source_tag = "#AI"
            elif source in {"python_org", "realpython", "pycharm_blog", "python_weekly"}:
                source_tag = "#Python"
            elif source in {"databricks", "confluent", "aws_bigdata"}:
                source_tag = "#DataEngineering"
            elif source in {"the_hacker_news", "gbhackers", "cybersecuritynews"}:
                source_tag = "#Security"
            elif source in {"github_blog", "vscode_updates", "docker_blog"}:
                source_tag = "#DevTools"
            else:
                source_tag = default_source_tag

            msg = format_news_message(
                url=url,
                content=content,
                topic_tag=topic_tag,
                source_tag=source_tag,
            )
            send_message(
                bot_token=settings.telegram_bot_token,
                chat_id=settings.telegram_chat_id,
                text=msg,
            )
            log_info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å (score={score:.3f}): {url}")

    def build_tools_digest_items(self, new_urls: List[str], max_tools: int = 5) -> List[dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—É–±–±–æ—Ç–Ω–µ–π –ø–æ–¥–±–æ—Ä–∫–∏ —Ç—É–ª–∑–æ–≤:
        –±–µ—Ä—ë–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ url, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ max_tools.
        –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç: {title, summary, url, use_case, source_tag}
        """
        if not new_urls:
            return []

        rows = get_news_by_urls(self.db_path, new_urls)
        if not rows:
            return []

        # rows: (url, title, summary, content, source, score)
        items = []
        for url, title, summary, content, source, score in rows:
            items.append(
                {
                    "url": url,
                    "title": (title or "–ù–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")[:120],
                    "summary": (summary or "").strip()[:250],
                    "source": source or "other",
                    "score": score or 0.0,
                }
            )

        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
        items.sort(key=lambda x: x["score"], reverse=True)
        items = items[:max_tools]

        # –¥–æ–±–∞–≤–ª—è–µ–º use_case –∏ source_tag
        for it in items:
            src = it["source"]
            it["use_case"] = build_tool_use_case(src)

            if src in {"github_blog", "vscode_updates", "docker_blog"}:
                it["source_tag"] = "#DevTools"
            elif src in {"python_org", "realpython", "pycharm_blog", "python_weekly"}:
                it["source_tag"] = "#Python"
            else:
                it["source_tag"] = "#Tools"

        return items

    def run_for_today(self) -> None:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: –¥–µ—Ä–≥–∞–µ—Ç—Å—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º —Ä–∞–∑ –≤ –¥–µ–Ω—å –≤ 09:00 –ø–æ –ú–æ—Å–∫–≤–µ.
        –ë—É–¥–Ω–∏: –æ–±—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏.
        –°—É–±–±–æ—Ç–∞: –ø–æ–¥–±–æ—Ä–∫–∞ —Ç—É–ª–∑–æ–≤.
        –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ: –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ–¥–µ–ª–∏.
        """
        weekday = datetime.now().weekday()
        plan = CONTENT_PLAN.get(weekday)
        if not plan:
            log_warning(f"–î–ª—è weekday={weekday} –Ω–µ—Ç –∫–æ–Ω—Ñ–∏–≥–∞ –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω–∞.")
            return

        log_info(f"–ó–∞–ø—É—Å–∫ –ü—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è weekday={weekday}")

        # 1. –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —Å–ø–∏—Å–∫—É —Å–∞–π—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –¥–Ω—è
        all_links = self.collect_links(plan.sites)

        # 2. –§–∏–ª—å—Ç—Ä—É–µ–º, –ø–∞—Ä—Å–∏–º, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏
        new_urls = self.fetch_and_store_new_articles_batch(
            links=all_links,
            substring=plan.substring,
            max_to_fetch=plan.max_fetch,
        )

        if weekday in {0, 1, 2, 3, 4}:
            # –ü–Ω‚Äì–ü—Ç ‚Äî –æ–±—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (—Ç–æ–ø-5 –ø–æ score)
            self.publish_top_news(new_urls, max_to_publish=5)
            log_info("–ó–∞–ø—É—Å–∫ –ü—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–±—É–¥–Ω–∏–π –¥–µ–Ω—å) –∑–∞–≤–µ—Ä—à—ë–Ω.")
        elif weekday == 5:
            # –°—É–±–±–æ—Ç–∞ ‚Äî –ø–æ–¥–±–æ—Ä–∫–∞ —Ç—É–ª–∑–æ–≤
            tools = self.build_tools_digest_items(new_urls, max_tools=5)
            from .telegram_bot import format_tools_digest_message

            if not tools:
                log_info("–ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç—É–ª–∑–æ–≤ –¥–ª—è —Å—É–±–±–æ—Ç–Ω–µ–π –ø–æ–¥–±–æ—Ä–∫–∏.")
            else:
                msg = format_tools_digest_message(tools)
                send_message(
                    bot_token=settings.telegram_bot_token,
                    chat_id=settings.telegram_chat_id,
                    text=msg,
                )
                log_info("–°—É–±–±–æ—Ç–Ω—è—è –ø–æ–¥–±–æ—Ä–∫–∞ —Ç—É–ª–∑–æ–≤ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞.")
        elif weekday == 6:
            # –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ ‚Äî –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ–¥–µ–ª–∏
            events = self.build_weekly_digest_items(days_back=7, limit=8)
            from .telegram_bot import format_weekly_digest_message

            msg = format_weekly_digest_message(events)
            send_message(
                bot_token=settings.telegram_bot_token,
                chat_id=settings.telegram_chat_id,
                text=msg,
            )
            log_info("–í–æ—Å–∫—Ä–µ—Å–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–µ–¥–µ–ª–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω.")

    def build_weekly_digest_items(self, days_back: int = 7, limit: int = 8) -> List[dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ø —Å–æ–±—ã—Ç–∏–π –∑–∞ N –¥–Ω–µ–π –¥–ª—è –≤–æ—Å–∫—Ä–µ—Å–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ {title, summary, url, source_tag}.
        """
        from .db import (
            get_top_news_for_period,  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö
        )

        rows = get_top_news_for_period(self.db_path, days_back=days_back, limit=limit)
        if not rows:
            return []

        items = []
        for url, title, summary, content, source, score, fetched_at in rows:
            src = source or "other"

            if src in {"openai", "anthropic", "huggingface", "stability_ai", "google_ai_blog"}:
                source_tag = "#AI"
            elif src in {"python_org", "realpython", "pycharm_blog", "python_weekly"}:
                source_tag = "#Python"
            elif src in {"databricks", "confluent", "aws_bigdata"}:
                source_tag = "#DataEngineering"
            elif src in {"the_hacker_news", "gbhackers", "cybersecuritynews"}:
                source_tag = "#Security"
            elif src in {"github_blog", "vscode_updates", "docker_blog"}:
                source_tag = "#DevTools"
            else:
                source_tag = "#–ù–æ–≤–æ—Å—Ç–∏IT"

            items.append(
                {
                    "url": url,
                    "title": (title or "–°–æ–±—ã—Ç–∏–µ –Ω–µ–¥–µ–ª–∏")[:140],
                    "summary": (summary or "").strip()[:260],
                    "source_tag": source_tag,
                    "score": score or 0.0,
                }
            )

        # —É–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ score –≤ SQL, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π:
        items.sort(key=lambda x: x["score"], reverse=True)
        return items
